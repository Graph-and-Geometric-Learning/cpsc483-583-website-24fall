{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Graphormer\n",
        "\n",
        "In this notebook, we'll train a graphormer (graph transformer) to perform a graph-level task on a molecule dataset. Specifically, we will be able to predict various attributes of molecules, including the toxicity of each molecule on 12 different targets, as well as the water solubility of each molecule.\n",
        "\n",
        "We will be using the OGB Molecule dataset for training and evaluation."
      ],
      "metadata": {
        "id": "KuqVw2Nri02a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Package installation\n",
        "\n",
        "We first install the required packages. Note that installation might take some time."
      ],
      "metadata": {
        "id": "omtz_S8ji3lW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pjyvYNerj_l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install ogb  # for datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cython"
      ],
      "metadata": {
        "id": "slX8w1tmi4zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cython"
      ],
      "metadata": {
        "id": "jwNznjAFi7Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "36dxdHrTi9Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we implement a few useful functions using cython to accelerate our pipeline. It is not required for you to read or understand the code below.\n"
      ],
      "metadata": {
        "id": "xavHHsdyi-tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cython\n",
        "\n",
        "import cython\n",
        "from cython.parallel cimport prange, parallel\n",
        "cimport numpy\n",
        "import numpy\n",
        "\n",
        "## Floyd Warshall computes the shortest path in a weighted graph\n",
        "def floyd_warshall(adjacency_matrix):\n",
        "\n",
        "    (nrows, ncols) = adjacency_matrix.shape\n",
        "    assert nrows == ncols\n",
        "    cdef unsigned int n = nrows\n",
        "\n",
        "    adj_mat_copy = adjacency_matrix.astype(long, order='C', casting='safe', copy=True)\n",
        "    assert adj_mat_copy.flags['C_CONTIGUOUS']\n",
        "    cdef numpy.ndarray[long, ndim=2, mode='c'] M = adj_mat_copy\n",
        "    cdef numpy.ndarray[long, ndim=2, mode='c'] path = numpy.zeros([n, n], dtype=numpy.int64)\n",
        "\n",
        "    cdef unsigned int i, j, k\n",
        "    cdef long M_ij, M_ik, cost_ikkj\n",
        "    cdef long* M_ptr = &M[0,0]\n",
        "    cdef long* M_i_ptr\n",
        "    cdef long* M_k_ptr\n",
        "\n",
        "    # set unreachable nodes distance to 510\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                M[i][j] = 0\n",
        "            elif M[i][j] == 0:\n",
        "                M[i][j] = 510\n",
        "\n",
        "    # floyed algo\n",
        "    for k in range(n):\n",
        "        M_k_ptr = M_ptr + n*k\n",
        "        for i in range(n):\n",
        "            M_i_ptr = M_ptr + n*i\n",
        "            M_ik = M_i_ptr[k]\n",
        "            for j in range(n):\n",
        "                cost_ikkj = M_ik + M_k_ptr[j]\n",
        "                M_ij = M_i_ptr[j]\n",
        "                if M_ij > cost_ikkj:\n",
        "                    M_i_ptr[j] = cost_ikkj\n",
        "                    path[i][j] = k\n",
        "\n",
        "    # set unreachable path to 510\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if M[i][j] >= 510:\n",
        "                path[i][j] = 510\n",
        "                M[i][j] = 510\n",
        "\n",
        "    return M, path\n",
        "\n",
        "\n",
        "def get_all_edges(path, i, j):\n",
        "    cdef unsigned int k = path[i][j]\n",
        "    if k == 0:\n",
        "        return []\n",
        "    else:\n",
        "        return get_all_edges(path, i, k) + [k] + get_all_edges(path, k, j)\n",
        "\n",
        "\n",
        "## Generates a 4D tensor of shape (num_nodes, num_nodes, max_path_length, edge_feature_size)\n",
        "## [i,j,k,:] stores the node features of the edge from path[k] and path[k+1] where path is the shortest path between i and j.\n",
        "def gen_edge_input(max_dist, path, edge_feat):\n",
        "\n",
        "    (nrows, ncols) = path.shape\n",
        "    assert nrows == ncols\n",
        "    cdef unsigned int n = nrows\n",
        "    cdef unsigned int max_dist_copy = max_dist\n",
        "\n",
        "    path_copy = path.astype(long, order='C', casting='safe', copy=True)\n",
        "    edge_feat_copy = edge_feat.astype(long, order='C', casting='safe', copy=True)\n",
        "    assert path_copy.flags['C_CONTIGUOUS']\n",
        "    assert edge_feat_copy.flags['C_CONTIGUOUS']\n",
        "\n",
        "    cdef numpy.ndarray[long, ndim=4, mode='c'] edge_fea_all = -1 * numpy.ones([n, n, max_dist_copy, edge_feat.shape[-1]], dtype=numpy.int64)\n",
        "    cdef unsigned int i, j, k, num_path, cur\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if path_copy[i][j] == 510:\n",
        "                continue\n",
        "            path = [i] + get_all_edges(path_copy, i, j) + [j]\n",
        "            num_path = len(path) - 1\n",
        "            for k in range(num_path):\n",
        "                edge_fea_all[i, j, k, :] = edge_feat_copy[path[k], path[k+1], :]\n",
        "\n",
        "    return edge_fea_all"
      ],
      "metadata": {
        "id": "z1NOI6RWi8bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 points)\n",
        "\n",
        "The code below defines some functions to preprocess the graphs in our dataset for use in the graphormer.\n",
        "\n",
        "In the relevant section of the code below, construct a Boolean adjacency matrix `adj` using the `edge_index` attribute of the input graph."
      ],
      "metadata": {
        "id": "mMgb2kTAjJoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "## This function is used to ensure that each feature is non-overlapping.\n",
        "def convert_to_single_emb(x, offset=512):\n",
        "    feature_num = x.size(1) if len(x.size()) > 1 else 1\n",
        "    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n",
        "    x = x + feature_offset\n",
        "    return x\n",
        "\n",
        "## Preprocesses each graph for use in model\n",
        "def preprocess_item(item):\n",
        "    ## item is a graph in our dataset.\n",
        "    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n",
        "\n",
        "    N = x.size(0)\n",
        "    x = convert_to_single_emb(x)\n",
        "\n",
        "    ## Question 1: Construct an adjacency matrix of size (N, N) named 'adj' and dtype=torch.bool using edge_index\n",
        "    ############# Your code here ############\n",
        "    ## (~2 lines of code)\n",
        "    #########################################\n",
        "\n",
        "\n",
        "    ## edge_attr is of shape (num_edges, edge_feature_dim)\n",
        "    ## attn_edge_type is of shape (N, N, edge_feature_dim)\n",
        "    if len(edge_attr.size()) == 1:\n",
        "        edge_attr = edge_attr[:, None]\n",
        "    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n",
        "    attn_edge_type[edge_index[0, :], edge_index[1, :]] = convert_to_single_emb(edge_attr) + 1\n",
        "\n",
        "    shortest_path_result, path = floyd_warshall(adj.numpy())\n",
        "    max_dist = np.amax(shortest_path_result)\n",
        "    edge_input = gen_edge_input(max_dist, path, attn_edge_type.numpy())\n",
        "    rel_pos = torch.from_numpy((shortest_path_result)).long()\n",
        "    attn_bias = torch.zeros(\n",
        "        [N + 1, N + 1], dtype=torch.float)  # + 1 to account for graph-level token\n",
        "\n",
        "    # Add the relevant tensors as attributes of the graph\n",
        "    item.x = x\n",
        "    item.adj = adj\n",
        "    item.attn_bias = attn_bias\n",
        "    item.attn_edge_type = attn_edge_type\n",
        "    item.rel_pos = rel_pos\n",
        "    item.in_degree = adj.long().sum(dim=1).view(-1)\n",
        "    item.out_degree = adj.long().sum(dim=0).view(-1)\n",
        "    item.edge_input = torch.from_numpy(edge_input).long()\n",
        "\n",
        "    return item"
      ],
      "metadata": {
        "id": "R8xApNzcjBpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test the function `preprocess_item` using the code below."
      ],
      "metadata": {
        "id": "Lx5xM2BxkWTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = torch.tensor([[0, 1, 1, 2],\n",
        "                       [1, 0, 2, 1]], dtype=torch.long)\n",
        "edge_attr = torch.rand(4, 3).long()\n",
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "data = preprocess_item(data)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "uq4WyOzMjLrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define some useful methods and the Batch class that will be used in the upcoming 'collate' function that pools the attributes of multiple graphs into a Batch that can be trained on."
      ],
      "metadata": {
        "id": "z5T6xOTxkdJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_1d_unsqueeze(x, padlen):\n",
        "    x = x + 1  # pad id = 0\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen], dtype=x.dtype)\n",
        "        new_x[:xlen] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_2d_unsqueeze(x, padlen):\n",
        "    x = x + 1  # pad id = 0\n",
        "    xlen, xdim = x.size()\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, xdim], dtype=x.dtype)\n",
        "        new_x[:xlen, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_attn_bias_unsqueeze(x, padlen):\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros(\n",
        "            [padlen, padlen], dtype=x.dtype).fill_(float('-inf'))\n",
        "        new_x[:xlen, :xlen] = x\n",
        "        new_x[xlen:, :xlen] = 0\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_edge_type_unsqueeze(x, padlen):\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, padlen, x.size(-1)], dtype=x.dtype)\n",
        "        new_x[:xlen, :xlen, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_rel_pos_unsqueeze(x, padlen):\n",
        "    x = x + 1\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype)\n",
        "        new_x[:xlen, :xlen] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_3d_unsqueeze(x, padlen1, padlen2, padlen3):\n",
        "    x = x + 1\n",
        "    xlen1, xlen2, xlen3, xlen4 = x.size()\n",
        "    if xlen1 < padlen1 or xlen2 < padlen2 or xlen3 < padlen3:\n",
        "        new_x = x.new_zeros([padlen1, padlen2, padlen3, xlen4], dtype=x.dtype)\n",
        "        new_x[:xlen1, :xlen2, :xlen3, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "class Batch():\n",
        "    def __init__(self, attn_bias, attn_edge_type, rel_pos, in_degree, out_degree, x, edge_input, y):\n",
        "        super(Batch, self).__init__()\n",
        "        self.in_degree, self.out_degree = in_degree, out_degree\n",
        "        self.x, self.y = x, y\n",
        "        self.attn_bias, self.attn_edge_type, self.rel_pos = attn_bias, attn_edge_type, rel_pos\n",
        "        self.edge_input = edge_input\n",
        "\n",
        "    def to(self, device):\n",
        "        self.in_degree, self.out_degree = self.in_degree.to(\n",
        "            device), self.out_degree.to(device)\n",
        "        self.x, self.y = self.x.to(device), self.y.to(device)\n",
        "        self.attn_bias, self.attn_edge_type, self.rel_pos = self.attn_bias.to(\n",
        "            device), self.attn_edge_type.to(device), self.rel_pos.to(device)\n",
        "        self.edge_input = self.edge_input.to(device)\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.in_degree.size(0)\n",
        "\n",
        "    def __str__(self):\n",
        "      return \"Batch(x={}, y={}, attn_bias={}, attn_edge_type={}, rel_pos={}, edge_input={})\".format(\n",
        "          self.x.size(),\n",
        "          self.y.size(),\n",
        "          self.attn_bias.size(),\n",
        "          self.attn_edge_type.size(),\n",
        "          self.rel_pos.size(),\n",
        "          self.edge_input.size()\n",
        "      )"
      ],
      "metadata": {
        "id": "ePh2gKY2kbaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to implement the `collator` function, which will be used to generate batches for training our graphormer. `collator` takes as input a set of graphs, and returns a `Batch` object that contains the relevant attributes of these graphs in a format that can be used for training."
      ],
      "metadata": {
        "id": "4QgtEZUnkjlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (5 points) and 3 (10 points)\n",
        "\n",
        "Question 2: Filters `items` to only include graphs that are not `None` and that have less nodes than `max_node`. Note that this should change `items` itself so it can be used later in the code.\n",
        "\n",
        "Question 3: Using a padding function defined above, pad `x` and `edge_inputs` upto the correct sizes, stipulated by `max_node` and `multi_hop_max_dist`. Ensure that you understand what `x` and `edge_inputs` are supposed to represent, their dimensions, and how they should be modified to be used in creating the `Batch` object to get this question correct.\n"
      ],
      "metadata": {
        "id": "hqrFFkeUkk9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(items, max_node=512, multi_hop_max_dist=20, rel_pos_max=20):\n",
        "\n",
        "    ## Question 2: Filter out graphs that are too large\n",
        "    ############# Your code here ############\n",
        "    ## (~1-2 lines of code)\n",
        "    #########################################\n",
        "\n",
        "    if len(items) == 0:\n",
        "        return None\n",
        "\n",
        "    ## Collect the relevant attributes of each graph\n",
        "    items = [(item.attn_bias, item.attn_edge_type, item.rel_pos, item.in_degree,\n",
        "              item.out_degree, item.x, item.edge_input[:, :, :multi_hop_max_dist, :], item.y) for item in items]\n",
        "    attn_biases, attn_edge_types, rel_poses, in_degrees, out_degrees, xs, edge_inputs, ys = zip(\n",
        "        *items)\n",
        "\n",
        "    ## Set the attention biases of nodes far away from the current node to -inf\n",
        "    ## This masks out distant nodes' attention contributions to the node at hand.\n",
        "    for idx, _ in enumerate(attn_biases):\n",
        "        attn_biases[idx][1:, 1:][rel_poses[idx] >= rel_pos_max] = float('-inf')\n",
        "\n",
        "    ## Find the largest number of nodes and largest distance to pad all other graphs' attributes to this length\n",
        "    max_node_num = max(i.size(0) for i in xs)\n",
        "    max_dist = max(i.size(-2) for i in edge_inputs)\n",
        "\n",
        "    y = torch.cat(ys)\n",
        "    ## Question 3: use the pad function to pad x and edge_inputs as used below\n",
        "    ############# Your code here ############\n",
        "    ## (~2-4 lines of code)\n",
        "    #########################################\n",
        "\n",
        "    attn_bias = torch.cat([pad_attn_bias_unsqueeze(\n",
        "        i, max_node_num + 1) for i in attn_biases])\n",
        "    attn_edge_type = torch.cat(\n",
        "        [pad_edge_type_unsqueeze(i, max_node_num) for i in attn_edge_types])\n",
        "    rel_pos = torch.cat([pad_rel_pos_unsqueeze(i, max_node_num)\n",
        "                        for i in rel_poses])\n",
        "    in_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
        "                          for i in in_degrees])\n",
        "    out_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
        "                           for i in out_degrees])\n",
        "\n",
        "    return Batch(\n",
        "        attn_bias=attn_bias,\n",
        "        attn_edge_type=attn_edge_type,\n",
        "        rel_pos=rel_pos,\n",
        "        in_degree=in_degree,\n",
        "        out_degree=out_degree,\n",
        "        x=x,\n",
        "        edge_input=edge_input,\n",
        "        y=y,\n",
        "    )"
      ],
      "metadata": {
        "id": "SW8WsI7Uki9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the following code to check your `collator` function:"
      ],
      "metadata": {
        "id": "4lTr3lOykqZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "edge_index1 = torch.tensor([[0, 1, 1, 2],\n",
        "                       [1, 0, 2, 1]], dtype=torch.long)\n",
        "edge_attr1 = torch.rand(4, 3).long()\n",
        "x1 = torch.tensor([[-1], [0], [1]], dtype=torch.long)\n",
        "y1 = torch.tensor([1], dtype=torch.float)\n",
        "edge_index2 = torch.tensor([[0, 1, 1, 2, 3, 1],\n",
        "                       [1, 0, 2, 1, 1, 3]], dtype=torch.long)\n",
        "edge_attr2 = torch.rand(6, 3).long()\n",
        "x2 = torch.tensor([[-1], [0], [1], [2]], dtype=torch.long)\n",
        "y2 = torch.tensor([0], dtype=torch.float)\n",
        "data1 = Data(x=x1, edge_index=edge_index1, edge_attr=edge_attr1, y=y1)\n",
        "data1 = preprocess_item(data1)\n",
        "data2 = Data(x=x2, edge_index=edge_index2, edge_attr=edge_attr2, y=y2)\n",
        "data2 = preprocess_item(data2)\n",
        "\n",
        "batch = collator([data1, data2])\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "fnpK1Lz9kndo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset + Dataloader\n",
        "\n",
        "In this section, we make a custom Dataset class for our graphs, and then define a dataloader that will be used in the training code."
      ],
      "metadata": {
        "id": "1i6QLyxUk2hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GraphormerDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "\n",
        "        self.num = len(dataset)\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.arange(self.num)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        ## Directly return the sampled graph\n",
        "        sampled_graph = self.dataset[self.indices[item]]\n",
        "        return preprocess_item(sampled_graph)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num\n",
        "\n",
        "    def shuffle(self):\n",
        "        rand = torch.randperm(self.num)\n",
        "        self.indices = self.indices[rand]"
      ],
      "metadata": {
        "id": "9XlGm0iRkrmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class GraphormerDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        self.dataset = GraphormerDataset(dataset)\n",
        "        self.collator = partial(collator, max_node=128, multi_hop_max_dist=5, rel_pos_max=1024)\n",
        "        kwargs[\"collate_fn\"] = self.__collate_fn__\n",
        "        super().__init__(dataset=self.dataset, **kwargs)\n",
        "\n",
        "    ## This function allows graphs to be grouped into batches during training, using the Batch class defined above\n",
        "    def __collate_fn__(self, batch):\n",
        "        batch_graphs = batch\n",
        "        batch_graphs = self.collator(batch_graphs)  # make the sampled graphs a batch\n",
        "        return batch_graphs"
      ],
      "metadata": {
        "id": "J0L0ckOOk30-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the Graphormer"
      ],
      "metadata": {
        "id": "Ed1cIMBlk5WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a few simple modules to define MLP and multi-head attention layers, combining them into a single transformer layer (`EncoderLayer`)."
      ],
      "metadata": {
        "id": "omFRsV_Dk6ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, ffn_size):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(hidden_size, ffn_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.layer2 = nn.Linear(ffn_size, hidden_size)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "## Implements multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, attention_dropout_rate, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.att_size = att_size = hidden_size // num_heads\n",
        "        self.scale = att_size ** -0.5\n",
        "\n",
        "        self.linear_q = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.linear_k = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.linear_v = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
        "\n",
        "        self.input_norm = nn.LayerNorm(hidden_size)\n",
        "        self.output_layer = nn.Linear(num_heads * att_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, attn_bias=None):\n",
        "        orig_q_size = x.size()\n",
        "\n",
        "        x = self.input_norm(x)\n",
        "\n",
        "        d_k = self.att_size\n",
        "        d_v = self.att_size\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
        "        q = self.linear_q(x).view(batch_size, -1, self.num_heads, d_k)\n",
        "        k = self.linear_k(x).view(batch_size, -1, self.num_heads, d_k)\n",
        "        v = self.linear_v(x).view(batch_size, -1, self.num_heads, d_v)\n",
        "\n",
        "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
        "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
        "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
        "\n",
        "        # Scaled Dot-Product Attention.\n",
        "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
        "        q = q * self.scale\n",
        "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
        "        if attn_bias is not None:\n",
        "            x = x + attn_bias\n",
        "\n",
        "        x = torch.softmax(x, dim=3)\n",
        "        x = self.att_dropout(x)\n",
        "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
        "        x = x.view(batch_size, -1, self.num_heads * d_v)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        assert x.size() == orig_q_size\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, ffn_size, dropout_rate, attention_dropout_rate, num_heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(hidden_size, attention_dropout_rate, num_heads)\n",
        "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.ffn = FeedForwardNetwork(hidden_size, ffn_size)\n",
        "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, attn_bias=None):\n",
        "        y = self.self_attn(x, attn_bias)\n",
        "        y = self.self_attention_dropout(y)\n",
        "        x1 = x + y\n",
        "        y = self.ffn(x1)\n",
        "        y = self.ffn_dropout(y)\n",
        "        return x1 + y ## Residual stream"
      ],
      "metadata": {
        "id": "2aZR7WAfk5Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (10 points)\n",
        "\n",
        "Implement the part of the forward method of the Graphormer that actually does the forward pass through the encoder layers. Remember that the `forward` method of each Encoder module takes as an argument an `attn_bias`. Also, remember to apply a final LayerNorm (`self.final_ln`) on the output of the Graphormer before returning the result."
      ],
      "metadata": {
        "id": "hes85kAFk__I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "## Initializes params randomly\n",
        "def init_bert_params(module, n_layers):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(n_layers))\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "class Graphormer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        num_heads,\n",
        "        hidden_dim,\n",
        "        dropout_rate,\n",
        "        input_dropout_rate,\n",
        "        ffn_dim,\n",
        "        edge_type,\n",
        "        multi_hop_max_dist,\n",
        "        attention_dropout_rate,\n",
        "        node_feature_num,\n",
        "        edge_feature_num\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        ## Generates an embedding for nodes and edges\n",
        "        self.atom_encoder = nn.Embedding(512 * node_feature_num + 1, hidden_dim, padding_idx=0) ## Assuming max_number of nodes is 512\n",
        "        self.edge_encoder = nn.Embedding(512 * edge_feature_num + 1, num_heads, padding_idx=0)\n",
        "\n",
        "        self.edge_type = edge_type\n",
        "        if self.edge_type == 'multi_hop':\n",
        "            self.edge_dis_encoder = nn.Embedding(128 * num_heads * num_heads, 1)\n",
        "        self.rel_pos_encoder = nn.Embedding(512, num_heads, padding_idx=0)\n",
        "        self.in_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
        "        self.out_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
        "        self.input_dropout = nn.Dropout(input_dropout_rate)\n",
        "\n",
        "        ## Define the layers of the transformer\n",
        "        encoders = [EncoderLayer(hidden_dim, ffn_dim, dropout_rate, attention_dropout_rate, num_heads)\n",
        "                    for _ in range(n_layers)]\n",
        "        self.layers = nn.ModuleList(encoders)\n",
        "        self.final_ln = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        ## Define the graph-level token\n",
        "        self.graph_token = nn.Embedding(1, hidden_dim)\n",
        "        self.graph_token_virtual_distance = nn.Embedding(1, num_heads)\n",
        "\n",
        "        self.multi_hop_max_dist = multi_hop_max_dist\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.apply(lambda module: init_bert_params(module, n_layers=n_layers))\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        attn_bias, rel_pos, x = batched_data.attn_bias, batched_data.rel_pos, batched_data.x\n",
        "        in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n",
        "        edge_input, attn_edge_type = batched_data.edge_input, batched_data.attn_edge_type\n",
        "\n",
        "        ## Construct the graph attention bias in multi-head attention\n",
        "        n_graph, n_node = x.size()[:2]\n",
        "        graph_attn_bias = attn_bias.clone()\n",
        "        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n",
        "            1, self.num_heads, 1, 1)  # [n_graph, n_head, n_node+1, n_node+1]\n",
        "\n",
        "        ## Construct the relative position encoding and add to graph attention bias\n",
        "        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
        "        rel_pos_bias = self.rel_pos_encoder(rel_pos).permute(0, 3, 1, 2)\n",
        "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + rel_pos_bias  ## Add spatial encoding\n",
        "        ## Include bias to attend to the graph-level token\n",
        "\n",
        "        t = self.graph_token_virtual_distance.weight.view(1, self.num_heads, 1)\n",
        "        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n",
        "        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n",
        "\n",
        "        ## Construct edge features to be used in MHA\n",
        "        if self.edge_type == 'multi_hop':\n",
        "            rel_pos_ = rel_pos.clone()\n",
        "            rel_pos_[rel_pos_ == 0] = 1  # set pad to 1\n",
        "            # set 1 to 1, x > 1 to x - 1\n",
        "            rel_pos_ = torch.where(rel_pos_ > 1, rel_pos_ - 1, rel_pos_)\n",
        "            if self.multi_hop_max_dist > 0:\n",
        "                rel_pos_ = rel_pos_.clamp(0, self.multi_hop_max_dist)\n",
        "                edge_input = edge_input[:, :, :, :self.multi_hop_max_dist, :]\n",
        "            # [n_graph, n_node, n_node, max_dist, n_head]\n",
        "            edge_input = self.edge_encoder(edge_input).mean(-2)\n",
        "            max_dist = edge_input.size(-2)\n",
        "            edge_input_flat = edge_input.permute(\n",
        "                3, 0, 1, 2, 4).reshape(max_dist, -1, self.num_heads)\n",
        "            edge_input_flat = torch.bmm(edge_input_flat, self.edge_dis_encoder.weight.reshape(\n",
        "                -1, self.num_heads, self.num_heads)[:max_dist, :, :])\n",
        "            edge_input = edge_input_flat.reshape(\n",
        "                max_dist, n_graph, n_node, n_node, self.num_heads).permute(1, 2, 3, 0, 4)\n",
        "            edge_input = (edge_input.sum(-2) /\n",
        "                          (rel_pos_.float().unsqueeze(-1))).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
        "            edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n",
        "\n",
        "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input  # Add edge encoder\n",
        "        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)\n",
        "\n",
        "        # node feauture + graph token\n",
        "        node_feature = self.atom_encoder(x).sum(dim=-2)  # [n_graph, n_node, n_hidden] after summing over node features\n",
        "\n",
        "        node_feature = node_feature + \\\n",
        "            self.in_degree_encoder(in_degree) + \\\n",
        "            self.out_degree_encoder(out_degree)  # degree encoder\n",
        "        graph_token_feature = self.graph_token.weight.unsqueeze(\n",
        "            0).repeat(n_graph, 1, 1)\n",
        "        graph_node_feature = torch.cat(\n",
        "            [graph_token_feature, node_feature], dim=1)\n",
        "\n",
        "        ## Question 4: Finish the forward pass\n",
        "        output = self.input_dropout(graph_node_feature)\n",
        "        ############# Your code here ############\n",
        "        ## (~2-4 lines of code)\n",
        "        #########################################\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2PMokqlLk9RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The following code can be used to check the forward pass:\n",
        "graphormer = Graphormer(n_layers=3,\n",
        "                       num_heads=5,\n",
        "                       hidden_dim=32,\n",
        "                       dropout_rate=0.1,\n",
        "                       input_dropout_rate=0.1,\n",
        "                       ffn_dim=32,\n",
        "                       edge_type=\"multi_hop\",\n",
        "                       multi_hop_max_dist=5,\n",
        "                       attention_dropout_rate=0.1,\n",
        "                       node_feature_num=2,\n",
        "                       edge_feature_num=3,\n",
        "                    )\n",
        "graphormer_output = graphormer(batch)\n",
        "print(graphormer_output.size())"
      ],
      "metadata": {
        "id": "ELty59GnlBv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder for graph-level tasks\n",
        "\n",
        "The graphormer generates node embeddings for each node including the graph-level token/node that is used for the graph-level task. However, we now have to construct a prediction/decoding head that translates this node embedding into the task output (e.g. solubility of a molecule). To do this, we construct `NNDecoder`, a prediction/classification head for all the tasks."
      ],
      "metadata": {
        "id": "hPTGeg2BlHtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (10 points)\n",
        "\n",
        "Implement both the `__init__` and `forward` methods of this decoder class, where the decoder is simply a linear layer from embedding to task space. Remember that we constructed a graph-level token for a reason - this should be used in the decoding."
      ],
      "metadata": {
        "id": "HMgU2sYOlI7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn.inits import uniform\n",
        "\n",
        "from torch_scatter import scatter_mean\n",
        "\n",
        "\n",
        "class NNDecoder(torch.nn.Module):\n",
        "    def __init__(self, num_tasks, emb_dim = 300, graph_pooling = \"mean\"):\n",
        "        super().__init__()\n",
        "        ############# Your code here ############\n",
        "        ## (~2 lines of code)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.decoder = torch.nn.Linear(self.emb_dim, num_tasks)\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, node_rep):\n",
        "      ## node_rep is the output of the Graphormer module (node embeddings of nodes in the graph for a batch of graphs)\n",
        "      ############# Your code here ############\n",
        "      ## (~2-3 lines of code)\n",
        "      #########################################"
      ],
      "metadata": {
        "id": "vhr-G7h8lFx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code to test the decoder:\n",
        "decoder = NNDecoder(2, 32)\n",
        "decoder_output = decoder(graphormer_output)\n",
        "print(decoder_output.size())"
      ],
      "metadata": {
        "id": "33_UoqsulKUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Graphormer\n",
        "\n",
        "It's finally time to write our train and test functions. We do this analogously to previous colabs."
      ],
      "metadata": {
        "id": "r3HcSeMHlNC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 (15 points)\n",
        "\n",
        "Finish the train function. Remember to zero gradients and step both optimizers for your transformer and for the decoder/prediction head. In calculating loss, make sure to use the correct criterion as defined earlier in the function. Finally, remember to only use datapoints with non-NaN y values in calculating your loss (mask the NaNs out of your calculation)."
      ],
      "metadata": {
        "id": "JS2rUnghlOvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "from time import time\n",
        "import numpy as np\n",
        "import logging\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### Importing OGB dataset\n",
        "from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "from ogb.graphproppred import Evaluator\n",
        "\n",
        "## The below train and test functions are able to deal with both classification and regression tasks, based on the string parameter of task_type\n",
        "## `'classfication' in task_type` means a classification task, otherwise a regression task\n",
        "def train(epoch, model_list, device, loader, optimizer_list, task_type):\n",
        "    model, decoder = model_list\n",
        "    optimizer, dec_optimizer = optimizer_list\n",
        "\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "\n",
        "    clf_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "    loss_list = []\n",
        "    epoch_iter = tqdm(loader, ncols=130)\n",
        "    for step, batch in enumerate(epoch_iter):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~10 lines of code)\n",
        "        #########################################\n",
        "        loss_list.append(loss.item())\n",
        "        epoch_iter.set_description(f\"epoch: {epoch}, train_loss: {loss:.4f}\")\n",
        "\n",
        "    return np.mean(loss_list)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model_list, device, loader, evaluator):\n",
        "    model, decoder = model_list\n",
        "\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            node_rep = model(batch)\n",
        "            pred = decoder(node_rep)\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ],
      "metadata": {
        "id": "ghEsD6iplLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're finally ready to train our model on datasets of molecules. First, we train on a dataset containing the solubility of molecules in water (a continuous variable)."
      ],
      "metadata": {
        "id": "mG9p6HyBlZnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plotting function for training loss and validation/test metrics\n",
        "def plot_curves(curves):\n",
        "    epochs = range(1, len(curves[\"train\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, curves[\"train\"], label='Training Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epochs)\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation and test metrics\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, curves[\"valid\"], label='Validation Metric', color='orange')\n",
        "    plt.plot(epochs, curves[\"test\"], label='Test Metric', color='green')\n",
        "    plt.title('Validation and Test Metrics over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xticks(epochs)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "## Code to train model on a particular task\n",
        "def train_model_on_task(task_name, model, node_feature_num, edge_feature_num, num_epochs=20, emb_dim=32, batch_size=32, lr_model=1e-4, lr_dec=1e-4):\n",
        "    device = \"cpu\"\n",
        "    s = time()\n",
        "\n",
        "    ## Load dataset and evaluator\n",
        "    dataset = PygGraphPropPredDataset(name=task_name)\n",
        "    evaluator = Evaluator(task_name)\n",
        "    split_idx = dataset.get_idx_split()\n",
        "\n",
        "    ## Only retain the first node_feature_num node features and first edge_feature_num edges\n",
        "    ## The # of node features in the dataset is 9 and edge features is 3, so can choose some slice of these features.\n",
        "    print(\"Dataset x shape:\", dataset.data.x.shape, \"Dataset edge attr shape:\", dataset.data.edge_attr.shape)\n",
        "    dataset.data.x = dataset.data.x[:,:node_feature_num]\n",
        "    dataset.data.edge_attr = dataset.data.edge_attr[:,:edge_feature_num]\n",
        "\n",
        "    ## Make the split dataloaders\n",
        "    train_loader = GraphormerDataLoader(dataset[split_idx[\"train\"]], batch_size=batch_size, shuffle=True, num_workers = 1)\n",
        "    valid_loader = GraphormerDataLoader(dataset[split_idx[\"valid\"]], batch_size=batch_size, shuffle=False, num_workers = 1)\n",
        "    test_loader = GraphormerDataLoader(dataset[split_idx[\"test\"]], batch_size=batch_size, shuffle=False, num_workers = 1)\n",
        "\n",
        "    ## Define model (transformer and prediction head/decoder) and optimizers\n",
        "    model = model.to(device)\n",
        "    decoder = NNDecoder(emb_dim = emb_dim, num_tasks = dataset.num_tasks).to(device)\n",
        "    model_list = [model, decoder]\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr_model)\n",
        "    dec_optimizer = optim.Adam(decoder.parameters(), lr=lr_dec)\n",
        "    optimizer_list = [optimizer, dec_optimizer]\n",
        "\n",
        "    train_curve = []\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_val_curve = []\n",
        "\n",
        "    ## Train the model and store loss/metrics\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_perf = train(epoch, model_list, device, train_loader, optimizer_list, dataset.task_type)\n",
        "        valid_perf = test(model_list, device, valid_loader, evaluator)\n",
        "        test_perf = test(model_list, device, test_loader, evaluator)\n",
        "        print({'Train Loss': train_perf, 'Validation Metric': valid_perf, 'Test Metric': test_perf})\n",
        "        train_curve.append(train_perf)\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "    if 'classification' in dataset.task_type:\n",
        "        ## For classification metrics, the higher the better\n",
        "        best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    else:\n",
        "        ## For regression metrics, the lower the better\n",
        "        best_val_epoch = np.argmin(np.array(valid_curve))\n",
        "\n",
        "    curves = {\"train\": train_curve, \"valid\": valid_curve, \"test\": test_curve}\n",
        "    print('Best Validation Metric: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test Metric: {}'.format(test_curve[best_val_epoch]))\n",
        "    return valid_curve[best_val_epoch], test_curve[best_val_epoch], curves\n",
        "\n",
        "def solubility():\n",
        "    ## Here, our task is to predict the solubility of molecules in water; this is a regression task.\n",
        "    ## Our metric is the mean-squared error between predicted and observed solubility values.\n",
        "    node_feature_num = 4\n",
        "    edge_feature_num = 2\n",
        "    model = Graphormer(n_layers=3,\n",
        "                       num_heads=5,\n",
        "                       hidden_dim=32,\n",
        "                       dropout_rate=0.1,\n",
        "                       input_dropout_rate=0.1,\n",
        "                       ffn_dim=32,\n",
        "                       edge_type=\"multi_hop\",\n",
        "                       multi_hop_max_dist=5,\n",
        "                       attention_dropout_rate=0.1,\n",
        "                       node_feature_num=node_feature_num,\n",
        "                       edge_feature_num=edge_feature_num,\n",
        "                    )\n",
        "    task_name = \"ogbg-molesol\"\n",
        "    return train_model_on_task(task_name, model, node_feature_num, edge_feature_num, num_epochs = 10)\n",
        "\n",
        "val_metric, test_metric, curves = solubility()\n",
        "plot_curves(curves)\n"
      ],
      "metadata": {
        "id": "THqqLOdYlXwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training pipeline is general enough that we can train our Graphormer on numerous other tasks, including all that tasks that are listed in this [spreadsheet](https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/master.csv); interpretations of these datasets can be found on page 15 of this [paper](https://arxiv.org/pdf/2307.04052.pdf).\n",
        "\n",
        "## Question 7 (15 points)\n",
        "\n",
        "Train your model on a toxicity dataset (Tox21) in a similar manner as above. The task here is to predict 12 binary labels for each molecule that correspond to its toxicity on 12 different targets. The metric used here is AUC. Adjust the hyperparameters of your model and/or training procedure to ensure that your test AUC is greater than 0.7. Plot the train loss and val/test metrics over epochs using the function defined above."
      ],
      "metadata": {
        "id": "pLKzdBxTle5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tox():\n",
        "    ## A function that trains the Graphormer on the toxicity dataset.\n",
        "    ############# Your code here ############\n",
        "    ## (~5-15 lines of code)\n",
        "    #########################################\n",
        "\n",
        "val_metric, test_metric, curves = tox()\n",
        "plot_curves(curves)\n"
      ],
      "metadata": {
        "id": "V95gNOZYlb3I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}