{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWhUwO_-mL2s"
      },
      "source": [
        "## Graph neural network basics\n",
        "\n",
        "In this Colab, we are going to introduce some basics of graph neural network (GNN) and build a pipeline for node classification tasks by PyTorch Geometric (PyG). See more introduction about [PyG](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R0-JXHoqhtD"
      },
      "source": [
        "## Outline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETQPVYpoqsvY"
      },
      "source": [
        "- Basic operation of PyG\n",
        "- Build a GNN by PyG For Node Classification\n",
        "- Link Prediction Task by Pyg\n",
        "- Graph Classification task by Pyg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRS0TB5mo7o"
      },
      "source": [
        "## Basic operation of PyG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-mwS1ClG9m5"
      },
      "outputs": [],
      "source": [
        "# import the pytorch library into environment and check its version\n",
        "import os\n",
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5DJkYNWGeL"
      },
      "source": [
        "Let's start installing PyG by `pip`. The version of PyG should match the current version of PyTorch. Here we follow the [instruction](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) of PyG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWtNbC9FgZOM"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install ogb  # for datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei5_O8ZXtlGb"
      },
      "source": [
        "### Create a Graph\n",
        "\n",
        "A single graph in PyG is described by an instance of `torch_geometric.data` which holds the some important attributes by default, like edge_index. We can easily create a graph of various number of edges and nodes by PyG. Take the following graph as an example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNeApmAljKyq"
      },
      "source": [
        "![](https://github.com/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/fig/graph_example.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSndMSIJhvC6"
      },
      "outputs": [],
      "source": [
        "# import torch_geometric.data into environment\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric import nn\n",
        "import torch_geometric.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Usm6XTk22d"
      },
      "source": [
        "We have 6 edges (undirected graph) and 3 nodes in this graph. So the edge index can be defined as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAqsazT4lPAZ"
      },
      "outputs": [],
      "source": [
        "edge_index = torch.tensor([[0, 1, 1, 2, 0, 2],\n",
        "                           [1, 0, 2, 1, 2, 0]], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku1lYh8eG9m9"
      },
      "source": [
        "Each edge is represented as a tuple (u, v), and that edge_index consists of num_edges columns where each column consists of the two indices u and v corresponding to each edge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fw5y3_Ulx1T"
      },
      "source": [
        "Besides, each node can have a node feature which describes the node's property:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3doHCX65l63N"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKFW9sRlnFK"
      },
      "source": [
        "Then we can define a `Data` object with edge index and node attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mmud5UqlfM9"
      },
      "outputs": [],
      "source": [
        "data = Data(x=x, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Af1qVLlmCNb"
      },
      "source": [
        "`Data` object supports many useful utility functions. For example, we can see the number of the nodes, and whether the graph is a undirected graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybw_wwuStyHo"
      },
      "outputs": [],
      "source": [
        "num_nodes = data.num_nodes\n",
        "print(\"number of nodes is:\", num_nodes)\n",
        "\n",
        "is_directed = data.is_directed()\n",
        "print(\"graph is directed or not:\", is_directed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCY73L2LGoS8"
      },
      "source": [
        "### Question 1 (5 points)\n",
        "\n",
        "What is the number of the neighbors of node 0 in the graph?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMg1NUP9Gnkq"
      },
      "outputs": [],
      "source": [
        "def get_n_neighbors(graph, idx):\n",
        "  # TODO: Implement a function that takes a Data object,\n",
        "  # an index of a node, and returns the number of the neighbors\n",
        "  # of this node (as an integer).\n",
        "\n",
        "  n_neighbors = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  #########################################\n",
        "\n",
        "  return n_neighbors\n",
        "\n",
        "idx = 0\n",
        "n_neighbors = get_n_neighbors(data, idx)\n",
        "print('Node with index {} has {} neighbors'.format(idx, n_neighbors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKHOYbMCE9b7"
      },
      "source": [
        "PyG has a number of graph data with various scales. Cora is one of the most famous dataset in graph learning, and we can use it by PyG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfZq2L2uEphL"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid('/tmp/cora', 'cora')\n",
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2h5lpJYpjV0"
      },
      "source": [
        "We can see the number of the nodes and edges in cora:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnYTDzQFpi40"
      },
      "outputs": [],
      "source": [
        "num_nodes = data.num_nodes\n",
        "print('cora has {} nodes'.format(num_nodes))\n",
        "\n",
        "num_edges = data.num_edges\n",
        "print('cora has {} edges'.format(num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HebClViZqU1V"
      },
      "source": [
        "### Question 2 (10 points)\n",
        "\n",
        "1. What is the number of the classes in cora dataset?\n",
        "2. Which node in Cora has the most number of neighbors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVuXmtFnqr4e"
      },
      "outputs": [],
      "source": [
        "def get_num_classes(data):\n",
        "  # TODO: Implement a function that takes a dataset object\n",
        "  # and returns the number of classes for that dataset.\n",
        "\n",
        "  num_classes = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  #########################################\n",
        "\n",
        "  return num_classes\n",
        "\n",
        "def get_idx_with_most_neighbors(data):\n",
        "  # TODO: Implement a function that takes a dataset object\n",
        "  # and returns the index of the node which has the most number of neighbors.\n",
        "\n",
        "  idx = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~3 line of code)\n",
        "  #########################################\n",
        "\n",
        "  return idx\n",
        "\n",
        "num_classes = get_num_classes(data)\n",
        "print(\"cora has {} classes\".format(num_classes))\n",
        "\n",
        "idx = get_idx_with_most_neighbors(data)\n",
        "print(\"{} in cora has the most number of neighbors\".format(idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiQW1Hs46Kpv"
      },
      "source": [
        "In cora, we split the data into train set, validation set and test set by node mask. All the nodes will participate in the message passing process, but we can only assess the train label during training process. This is what we call [transductive learning](https://en.wikipedia.org/wiki/Transduction_(machine_learning))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioXYJFV-7UU_"
      },
      "outputs": [],
      "source": [
        "node_feature = data.x\n",
        "\n",
        "train_node_feature = node_feature[data.train_mask]\n",
        "valid_node_feature = node_feature[data.val_mask]\n",
        "test_node_feature = node_feature[data.test_mask]\n",
        "\n",
        "print(\"number of nodes in train set,\", train_node_feature.shape[0])\n",
        "print(\"number of nodes in valid set,\", valid_node_feature.shape[0])\n",
        "print(\"number of nodes in test set,\", test_node_feature.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhLEhOimr7mY"
      },
      "source": [
        "## Build a GNN by PyG for Node Classification\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will use PyG to build a classic graph neural network called GCN([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). Then we will apply this model to handle node classification task in cora.\n",
        "A GCN is built by stacking multiple graph convolution layers `GCNConv` which passes the messages from neighbors to the center node. Here we can define a `GCNConv` by PyG:"
      ],
      "metadata": {
        "id": "sCD8AFXhXfGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-8eQfROu7Hm"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "conv = GCNConv(in_channels=1433, out_channels=200, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxNlpZ5cvI9s"
      },
      "source": [
        "`in_channels` is the dimension of node's input feature, `out_channels` is the  dimension of the output representation of node, and `normalize` is whether to add self-loops and compute symmetric normalization on the adjacent matrix.\n",
        "The feature's dimension in cora is 1433, so `in_channels` is set as 1433. We can perform a message passing on cora like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlwiHnm7wy8S"
      },
      "outputs": [],
      "source": [
        "node_feature = data.x\n",
        "edge_index = data.edge_index\n",
        "\n",
        "node_representation = conv(node_feature, edge_index)\n",
        "\n",
        "print(\"dimension of node_feature:\", node_feature.shape)\n",
        "print(\"dimension of node_representation:\", node_representation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxItCC52xud5"
      },
      "source": [
        "We can see that the inputs of `GCNConv` are node feature and edge index. Then the convolution module will perform a message passing like GCN.\n",
        "Recall the MLP we build in colab0. Here we also use `nn.Module` to define a MLP class containing the basic modules of GCN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eug5Vv-Py5f_"
      },
      "source": [
        "### Question 3 (5 points)\n",
        "\n",
        "Following the instruction and build a GCN class using the `GCNConv` modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDO3ucqjtBF6"
      },
      "outputs": [],
      "source": [
        "from numpy import ERR_DEFAULT\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Define two GCNConv modules and a ReLU function.\n",
        "        # The input size and output size of first GCNConv module should be in_channels and hidden_channels\n",
        "        # The input size and output size of second GCNConv module should be hidden_channels and out_channels\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, node_feature, edge_index):\n",
        "\n",
        "        output = None\n",
        "\n",
        "        # TODO: Use the modules you define in __init__ to perform message passing.\n",
        "        # ReLU function should be used in the middle of two GCNConv modules.\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "        #########################################\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHgZ1ozBsYXN"
      },
      "source": [
        "### Training and Testing\n",
        "\n",
        "Now we can try to construct training and testing pipeline, which is similar to what we do in colab1. First we initialize a GCN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0hojVFN1iMm"
      },
      "outputs": [],
      "source": [
        "hidden_channels = 200\n",
        "num_features = dataset.num_features\n",
        "num_classes = get_num_classes(data) # please write down the number of classes\n",
        "\n",
        "model = GCN(num_features, hidden_channels, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mA96Hyl2B_v"
      },
      "source": [
        "Then we define the optimizer and loss function. Since it is a classification task, we use Cross Entropy Loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPOFix9C2BRb"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCbOTE893mR6"
      },
      "source": [
        "### Question 4 (10 points)\n",
        "\n",
        "Please follow the instruction and implement a function that trains a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45-zRkbN3lxF"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, loss_fn):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # TODO: Define train function.\n",
        "    # 1. put the model into train mode\n",
        "    # 2. clear the gradients calculated from the last batch\n",
        "    # 3. get the prediction by model\n",
        "    # 4. calculate the loss between our predictions and the actual labels.\n",
        "    # Just using nodes in train set!\n",
        "    # 5. calculate the gradients of each parameter\n",
        "    # 6. update the parameters by taking an optimizer step\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~7 line of code)\n",
        "    #########################################\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxgTvA6U_3-B"
      },
      "source": [
        "### Question 5 (10 points)\n",
        "\n",
        "Please follow the instruction and implement a function that evaluates a model in train, valid and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E60aSasX_-Qe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "\n",
        "    accuracy_list = [0, 0, 0]\n",
        "\n",
        "    # TODO: Define test function.\n",
        "    # 1. put the model into eval mode\n",
        "    # 2. get the prediction by model\n",
        "    # 3. calculate the accuracy for each set\n",
        "    # NOTE: the results should be a list containing the accuracy of different set\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~5 line of code)\n",
        "    #########################################\n",
        "\n",
        "    return accuracy_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDrWoNtBbpi"
      },
      "source": [
        "We can start to train our model with `train` and `test` functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsLEBfWDBu2k"
      },
      "outputs": [],
      "source": [
        "hidden_channels = 200\n",
        "num_features = dataset.num_features\n",
        "num_classes = get_num_classes(data) # please write down the number of classes\n",
        "\n",
        "model = GCN(num_features, hidden_channels, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 50\n",
        "best_val_acc = final_test_acc = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train(model, data, optimizer, loss_fn)\n",
        "    #print(loss)\n",
        "    train_acc, val_acc, test_acc = test(model, data)\n",
        "    #print(train_acc, val_acc, test_acc)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        final_test_acc = test_acc\n",
        "print(\"after {} epochs' training, the best test accuracy is {}\".format(epochs, final_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It seems that our current hyperparameters, including the choice of optimizer algorithm, learning rate, and the number of training epochs, are not yielding satisfactory performance. To address this issue, we need to fine-tune these parameters for improved results.\n",
        "\n",
        "One widely adopted approach for hyperparameter tuning involves systematically exploring various options for each hyperparameter. This method, often referred to as hyperparameter optimization, can significantly enhance the performance of our model.\n",
        "\n",
        "For more in-depth information, you can take a look at the following methods:\n",
        "\n",
        "\n",
        "1.   [\n",
        "Bayesian Optimization for Hyperparameter Tuning.](https://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf)\n",
        "2.   Grid Search and Random Search."
      ],
      "metadata": {
        "id": "yGaD7peuKSvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NRDEljKwGite"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please incorporate any values and methods that you believe are worth considering into the following lists."
      ],
      "metadata": {
        "id": "49vLQipiMS_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-3, 1e-4, 1e-5, 1e-6]\n",
        "epochs_list = [10, 20, 40, 60, 80, 100]\n",
        "optimizers = [optim.Adam, optim.SGD, optim.SGD]  # Add more optimizers as needed\n",
        "optimizer_names = ['Adam', 'SGD', 'Nesterov']  # Match optimizer list"
      ],
      "metadata": {
        "id": "vfBS8jEzHMS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we establish a Pandas DataFrame schema to store and subsequently visualize the corresponding learning rates based on the input hyperparameters, as follows:"
      ],
      "metadata": {
        "id": "qR0ub8ZoM4wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Learning Rate', 'Epochs', 'Optimizer', 'Final Test Accuracy']\n",
        "results_df = pd.DataFrame(columns=columns)"
      ],
      "metadata": {
        "id": "I3AjPGgGHaqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6 (10 points)\n",
        "\n",
        "Please complete the for loops below to reset the model at each index, set the hyperparameters, and calculate the resulting accuracy, which will then be recorded in the `results_df` dataframe."
      ],
      "metadata": {
        "id": "AWHlWm4qNa-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in learning_rates:\n",
        "    for epochs in epochs_list:\n",
        "        for opt, opt_name in zip(optimizers, optimizer_names):\n",
        "            model = GCN(num_features, hidden_channels, num_classes)\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            best_val_acc = final_test_acc = 0\n",
        "\n",
        "            ############# Your code here ############\n",
        "            ## (~7 line of code)\n",
        "            #########################################\n",
        "\n",
        "            # Store the results in the DataFrame\n",
        "            new_row = pd.DataFrame({'Learning Rate': [lr],\n",
        "                                    'Epochs': [epochs],\n",
        "                                    'Optimizer': [opt_name],\n",
        "                                    'Final Test Accuracy': [final_test_acc]})\n",
        "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "VAtE7JApHg2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can visualize the data in `results_df` to identify suitable values for the hyperparameters."
      ],
      "metadata": {
        "id": "475gmalgORJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting accuracy vs. epochs for each learning rate and optimizer\n",
        "for lr in learning_rates:\n",
        "    for opt_name in optimizer_names:\n",
        "        subset_df = results_df[(results_df['Learning Rate'] == lr) & (results_df['Optimizer'] == opt_name)]\n",
        "        plt.plot(subset_df['Epochs'], subset_df['Final Test Accuracy'], label=f'{opt_name}, LR={lr}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Final Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ycE_gu4OJtZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link Prediction\n"
      ],
      "metadata": {
        "id": "9MJkv7UOG1hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Dataset preprocess\n",
        "\n",
        "As shown in the following figure, link prediction is to predict whether two nodes in a graph have a link, which can be considered as a binary classification task. We will construct a link prediction dataset containing training, validation, and test set based on Cora."
      ],
      "metadata": {
        "id": "IkIDVqdlMjtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<center>\n",
        "<img src=\"https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/01/link-prediction-graphical-neural-network-1024x576.webp?resize=1024%2C576&ssl=1\" height=\"200\" width=\"350\"/>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dra9qV4FOXUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a graph, we divide the initial edge set into three distinct edge sets which represent the training, validation, and test set. Training set and validation set share a same graph structure. Test set contains some edges which does not exist in training and validation set to prevent data leakage.\n",
        "<!-- Training set does not include edges in validation and test set, and the validation split does not include edges in the test split. Validation and test data should not be leaked into the training set. -->"
      ],
      "metadata": {
        "id": "6Q4xAy2UMnpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model will be optimized on the training set. We can use `transforms` function in PyG to easily generate the data splits:"
      ],
      "metadata": {
        "id": "E-CmT5VIPMXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomLinkSplit(num_val=0.05,  # ratio of edges including in the validation set\n",
        "                      num_test=0.2,  # ratio of edges including in the test set\n",
        "                      is_undirected=True,\n",
        "                      add_negative_train_samples=False),\n",
        "])"
      ],
      "metadata": {
        "id": "RGguSS3FMnUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Cora dataset:"
      ],
      "metadata": {
        "id": "fE4X5EJ0PR5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid('/tmp/cora', 'cora', transform=transform)"
      ],
      "metadata": {
        "id": "2a9YBN3UHFqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data will be transformed from a data object to three tuples, where each element represents the corresponding split:"
      ],
      "metadata": {
        "id": "S_TeL5viPatJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = dataset[0]"
      ],
      "metadata": {
        "id": "QjzrxhmwHAj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now data object has two attributes of edge: `edge_index` and `edge_label_index`. `edge_index` denotes the graph structure used for performing message passing in GNN. `edge_label_index` denotes the edge index used to calculate loss in training set, or to evaluate the model in validation and test set.\n"
      ],
      "metadata": {
        "id": "Ze6ILR13Pfpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the statistics of data:"
      ],
      "metadata": {
        "id": "VhIvYI-lPfkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of the nodes in training, validation and test data are\", train_data.num_nodes, val_data.num_nodes, test_data.num_nodes)\n",
        "print(\"Number of the edges in training, validation and test data are\", train_data.num_edges, val_data.num_edges, test_data.num_edges)\n",
        "print(\"Number of the edge_label_index in training, validation and test data are\", train_data.edge_label_index.shape[1],\n",
        "                                                                                  val_data.edge_label_index.shape[1],\n",
        "                                                                                  test_data.edge_label_index.shape[1])"
      ],
      "metadata": {
        "id": "Zh9CojsAPjU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "I6bdhnhPPmBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the same GCN you constrcted."
      ],
      "metadata": {
        "id": "G3GncxoXQBwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(dataset.num_features, hidden_channels=128, out_channels=64)"
      ],
      "metadata": {
        "id": "wX5M2f1gQREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "BKR1W7rMQTpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar as the what we do in the node classification task, we first apply the GCN model to produce the representation of each node in the graph. Usually we will use **inner product** to measure the similarity between two node representations to determine how likely it is for these two nodes to be connected."
      ],
      "metadata": {
        "id": "ycOaNK_1Qc8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 7 (5 points)\n",
        "\n",
        "Following the instruction and implement the function to calculate the inner product:"
      ],
      "metadata": {
        "id": "XkmpKFhoQfXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(node_embs, edge_index):\n",
        "    result = 0\n",
        "\n",
        "    # TODO: Define similarity function.\n",
        "    # 1. calculate the inner product between all the pairs in the edge_index\n",
        "    # Note: the shape of node_embs is [n, h] where n is the number of nodes, and h is the embedding size\n",
        "    # the shape of edge_index is [2, m] where m is the number of edges\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~1 line of code)\n",
        "    #########################################\n",
        "\n",
        "    return result\n",
        "\n",
        "n, h = 5, 10  # number of nodes and embedding size\n",
        "node_embs = torch.rand(n, h)\n",
        "edge_index = torch.tensor([[0, 1, 2, 3],\n",
        "                           [2, 3, 0, 1]])  # compute the similarity of (0, 2), (1, 3), (2, 0), (3, 1)\n",
        "similarity = compute_similarity(node_embs, edge_index)\n",
        "print(\"Similairty:\", similarity)"
      ],
      "metadata": {
        "id": "qRUjKuvJQbLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We optimize the model by minimizing the loss function. Here we consider the link prediction task as a binary classification task (edge exists or no), and apply binary cross entropy loss:"
      ],
      "metadata": {
        "id": "Llh52gpiQxLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "m_ue1HNBQw-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The edges in the graph will be taken as the positive examples with label=1 in the loss function. To prevent model from collapse, we usually will feed some **negative examples** to the loss function, which is the non-existing edges in the graph. The number of negative examples should equal to the number of positive ones."
      ],
      "metadata": {
        "id": "_olpc51XQ3hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of PyG, we can easily perform the negative sampling. Here is an example:"
      ],
      "metadata": {
        "id": "7HkVK7WWQ7-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "neg_edge_index = negative_sampling(\n",
        "      edge_index=train_data.edge_index,  # positive edges in the graph\n",
        "      num_nodes=train_data.num_nodes,  # number of nodes\n",
        "      num_neg_samples=5,  # number of negative examples\n",
        "    )\n",
        "\n",
        "print(\"shape of neg_edge_index:\", neg_edge_index.shape)  # [2, num_neg_samples]\n",
        "print(\"negative examples:\", neg_edge_index)"
      ],
      "metadata": {
        "id": "-wn9IvtPQoZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive examples (`edge_label_index`) will be assigned the label 1, and negative ones will be assigned the label 0. We can obtain the label of positive examples like this:"
      ],
      "metadata": {
        "id": "gVCP-wKERAwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"positive examples' labels:\", train_data.edge_label)"
      ],
      "metadata": {
        "id": "zCg0x1tyQ-Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can construct training and testing pipeline."
      ],
      "metadata": {
        "id": "MpuprVxhRQFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 8 (15 points)\n",
        "\n",
        "Please follow the instruction and implement a function that trains a model."
      ],
      "metadata": {
        "id": "EsfaRDYKRYf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, loss_fn):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # TODO: Define train function.\n",
        "    # 1. put the model into train mode\n",
        "    # 2. clear the gradients calculated from the last batch\n",
        "    # 3. use 'edge_index' to get the node representation by model\n",
        "    # 4. sample the negative examples with the same number of positive ones (edge_label_index)\n",
        "    # 5. concatenate the positive edges and negative edges\n",
        "    # 6. concatenate the labels of positive edges and negative edges\n",
        "    # 7. calculate the similarity between two end nodes to determine the probability that the corresponding edge is present on the graph.\n",
        "    # 8. feed the probability and edge label to the loss function\n",
        "    # 9. calculate the gradients of each parameter\n",
        "    # 10. update the parameters by taking an optimizer step\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~10 line of code)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "11oIZg5VRCqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We usually use [AUC score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) to evaluate the performance of model on binary classification task. The test function is as followed:"
      ],
      "metadata": {
        "id": "FZrd0esJRiEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)  # use `edge_index` to perform message passing\n",
        "    out = compute_similarity(out, data.edge_label_index).view(-1).sigmoid()  # use `edge_label_index` to compute the loss\n",
        "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
      ],
      "metadata": {
        "id": "PC7hxxZNRe-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start to train our model based on `train` and `test` function:"
      ],
      "metadata": {
        "id": "tLkW7ZlBRnXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "best_val_auc = final_test_auc = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train(model, train_data, optimizer, loss_fn)\n",
        "    valid_auc = test(model, val_data)\n",
        "    test_auc = test(model, test_data)\n",
        "    if valid_auc > best_val_auc:\n",
        "        best_val_auc = valid_auc\n",
        "        final_test_auc = test_auc\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {valid_auc:.4f}, Test: {test_auc:.4f}')"
      ],
      "metadata": {
        "id": "rCVflGNBRnCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph classification task"
      ],
      "metadata": {
        "id": "0zjMPRTBUI6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's have a closer look at the task of graph classification. Graph classification refers to the problem of classifiying entire graphs, given a dataset of graphs. Here, we will apply GNN to embed entire graphs."
      ],
      "metadata": {
        "id": "NNWEEvXsUTWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preprocess"
      ],
      "metadata": {
        "id": "LBJFZz7HUN7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most common benchmark dataset of graph classification is [TUDatasets](https://chrsmrrs.github.io/datasets/) which are collected by TU Dortmund University. Each graph in this dataset is a molecule, and the task is to infer whether a molecule inhibits HIV virus replication or not. We can load this dataset by PyG. In this colab, we mainly focus on one of the smaller ones in TUDatasets: MUTAG dataset."
      ],
      "metadata": {
        "id": "eFOOZqkKUg-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='/tmp/mutag', name='MUTAG')\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "p3T4zuXkUhhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can obtain its number of graphs, classes, node features:"
      ],
      "metadata": {
        "id": "CFGKvYLPUrh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'number of graphs: {len(dataset)}')\n",
        "print(f'number of classes: {dataset.num_classes}')\n",
        "print(f'Number of node features: {dataset.num_node_features}')"
      ],
      "metadata": {
        "id": "pUUma7g1UkkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There 188 graphs in this dataset, and we can get the graph object with any id. For example:"
      ],
      "metadata": {
        "id": "kqcs7ZpiVK-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[5]\n",
        "print(f'5-th graph object: {data}')"
      ],
      "metadata": {
        "id": "VyKmgfGaVItA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can obtain some statistics for each graph object:"
      ],
      "metadata": {
        "id": "c7TcYWAyVQfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "WrvpADVwVUOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will divide the dataset into training set and test set, and there is no duplicate graph in these two sets. We can randomly pick 150 graphs to form training set, and the remaining ones will be the test set:"
      ],
      "metadata": {
        "id": "stZ95bWOVXpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]"
      ],
      "metadata": {
        "id": "OUXdRNRDVTwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-batching of graphs\n",
        "\n",
        "To fully utilize GPU, we will conduct mini-batch training which can be achieved by PyG. A batch of graphs will be grouped in a giant graph that holds multiple isolated subgraphs, and node features are simply concatenated. `dataloader` object in PyG can easily finish the aboved process:"
      ],
      "metadata": {
        "id": "SRPQjKq3VcfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "Gohzyqz9VaOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example to show how dataloader works. We can observe that multiple graphs are included in a giant graph."
      ],
      "metadata": {
        "id": "tygJb0_JVhz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}, number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(f'Step {step + 1}, number of nodes in the current batch: {data.num_nodes}')\n",
        "    print(f'Step {step + 1}, the graph id to which each node belongs is: {data.batch}')\n",
        "    print()"
      ],
      "metadata": {
        "id": "Yf8GmElHVf2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph id of every node to which it belongs is indicated by the `batch` attribute."
      ],
      "metadata": {
        "id": "3-Z6D2YlVoP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Implementation"
      ],
      "metadata": {
        "id": "KpoNXddcVtgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we perform message passing to embed each node in the graph, then aggregate the node embeddings into a graph embedding by pooling method. Finally the graph embedding will be fed to a classifier to conduct graph classification."
      ],
      "metadata": {
        "id": "ghqyQjHdVvns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply mean pooling method which is to simply take the average of node embeddings. Here is an example of mean pooling:"
      ],
      "metadata": {
        "id": "RuGdNqNQVyOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "x = torch.rand(5, 4)  # embeddings of 5 nodes\n",
        "\n",
        "# graph id. The first two nodes belong to first graph,\n",
        "# the 3rd node belongs to the second graph,\n",
        "# and the last two nodes belong to the last graph\n",
        "batch = torch.tensor([0, 0, 1, 2, 2])\n",
        "\n",
        "x = global_mean_pool(x, batch)  # node embedding and the graph id to which each node belongs to\n",
        "print(f\"shape of graph embedding: {x.shape}\")"
      ],
      "metadata": {
        "id": "BMfuWpLVVk1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 9 (15 points)\n",
        "\n",
        "Follow the instructions and implement the GNN model for graph classifiation task."
      ],
      "metadata": {
        "id": "XVJgmw6vV-oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # TODO: Define two GCNConv modules, a linear classifier and a ReLU function.\n",
        "        # The input size and output size of first GCNConv module should be in_channels and hidden_channels\n",
        "        # The input size and output size of second GCNConv module should be hidden_channels and hidden_channels\n",
        "        # The input size and output size of Linear module should be hidden_channels and out_channels\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~4 line of code)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "\n",
        "        output = None\n",
        "\n",
        "        # TODO: Use the modules you define in __init__ to perform message passing.\n",
        "        # ReLU function should be used in the middle of two GCNConv modules.\n",
        "        # Apply global_mean_pool module to generate graph embeddings\n",
        "        # Apply linear classifier to predict the label\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "\n",
        "        #########################################\n",
        "        return output"
      ],
      "metadata": {
        "id": "NG2OKZpVV1Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model and the optimizer:"
      ],
      "metadata": {
        "id": "p03op5mOWPbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(in_channels=dataset.num_node_features, hidden_channels=64, out_channels=dataset.num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "hNz0Jc39WLtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "Y7BewqgIWWO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use cross entropy loss to optimize:"
      ],
      "metadata": {
        "id": "VAVDlVLoWYnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "c9NgCD26WTq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 10 (15 points)\n",
        "\n",
        "Now we are going to implement `train` function. Please follow the instrution:"
      ],
      "metadata": {
        "id": "WOKSDCGMWgeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, loss_func):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # TODO: Define train function.\n",
        "    # 1. put the model into train mode\n",
        "    # 2. iterate over the dataloader\n",
        "    # 3. obtain the predicted result by model\n",
        "    # 4. compute the loss\n",
        "    # 5. loss backward\n",
        "    # 6. update the parameters by taking an optimizer step\n",
        "    # 7. clear the gradients calculated from the last batch\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~7 line of code)\n",
        "    #########################################\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rbjB7RYlWcRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `test` function is implemented as followed:"
      ],
      "metadata": {
        "id": "4rQwvA-rWmfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
      ],
      "metadata": {
        "id": "vQMKr_ZLWkT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train and evaluate our model on graph classification task:\n"
      ],
      "metadata": {
        "id": "MPZ2gbIoWqMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "for epoch in range(1, epochs):\n",
        "    model = train(model, train_loader, optimizer, loss_func)\n",
        "    test_acc = test(model, test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "ST8lu7A-WoxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "Make sure to run all the cells and save a copy of this colab in your driver. If you complete this notebook, download the colab and upload your work to canvas to submit it."
      ],
      "metadata": {
        "id": "dSRwQfuQPu9P"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}